{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "import gzip\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f=gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./train_Category.json.gz'\n",
    "reviews=[]\n",
    "for l in readGz(path):\n",
    "    reviews.append(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=reviews[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_votes': 0,\n",
       " 'review_id': 'r99763621',\n",
       " 'user_id': 'u17334941',\n",
       " 'review_text': \"Genuinely enthralling. If Collins or Bernard did invent this out of whole cloth, they deserve a medal for imagination. Lets leave the veracity aside for a moment - always a touchy subject when it comes to real life stories of the occult - and talk about the contents. \\n The Black Alchemist covers a period of two years in which Collins, a magician, and Bernard, a psychic, undertook a series of psychic quests that put them in opposition with the titular Black Alchemist. As entertainment goes, the combination of harrowing discoveries, ancient lore, and going down the pub for a cigarette and a Guinness, trying to make sense of it all while a hen party screams at each other, is a winner. It is simultaneously down to earth and out of this world. \\n It reads fast, both because of the curiousity and because Collins has a very clear writing style. Sometimes its a little clunky or over repetitive and there's a few meetings that get underreported, but I am very much quibbling here. Mostly important, he captures his own and Bernard's sense of wonder, awe and occasionally revulsion enough that I shared them.\",\n",
       " 'rating': 5,\n",
       " 'genreID': 2,\n",
       " 'genre': 'fantasy_paranormal'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "bigramDic=defaultdict(int)\n",
    "unigramDic=defaultdict(int)\n",
    "for r in review:\n",
    "    r=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    for n in range(len(r)-1):\n",
    "        bigram=str(r[n]+' '+r[n+1])\n",
    "        unigramDic[r[n]]+=1\n",
    "        bigramDic[bigram]+=1\n",
    "    unigramDic[r[len(r)-1]]+=1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=[(bigramDic[x], x) for x in bigramDic]\n",
    "counts.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7927, 'of the'),\n",
       " (5850, 'this book'),\n",
       " (5627, 'in the'),\n",
       " (3189, 'and the'),\n",
       " (3183, 'is a')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###festures: 1000 most common bigrams\n",
    "###label: rating\n",
    "###MSE: regularized regression 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=1000\n",
    "count=counts[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_index=dict(zip([x[1] for x in count],range(1000)))\n",
    "bigrams=[x[1] for x in count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(r):\n",
    "    feat=[0]*1000\n",
    "    r_ls=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    for n in range(len(r_ls)-1):\n",
    "        bi_str=str(r_ls[n]+\" \"+r_ls[n+1])\n",
    "        if bi_str in bigrams:\n",
    "            feat[bigram_index[bi_str]]+=1\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[]\n",
    "for r in review:\n",
    "    feat=feature(r)\n",
    "    features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[r['rating'] for r in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=False, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regularized regression\n",
    "from sklearn import linear_model\n",
    "clf=linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=clf.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0178804824879226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE=mean_squared_error(labels, predictions)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE if 1.01788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000 features will be some combination of unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_counts=[(bigramDic[x],x) for x in bigramDic]\n",
    "uni_counts=[(unigramDic[x],x) for x in unigramDic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts=bi_counts+uni_counts\n",
    "all_counts.sort(reverse=True)\n",
    "all_count=all_counts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count_index=dict(zip([x[1] for x in all_count],range(len(all_count))))\n",
    "all_count_words=list(all_count_index.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature2(r):\n",
    "    feat=[0]*1000\n",
    "    r_ls=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    for n in range(len(r_ls)-1):\n",
    "        bigram=str(r_ls[n]+' '+r_ls[n+1])\n",
    "        unigram=r_ls[n]\n",
    "        if bigram in all_count_words:\n",
    "            feat[all_count_index[bigram]]+=1\n",
    "        if unigram in all_count_words:\n",
    "            feat[all_count_index[unigram]]+=1\n",
    "    if r_ls[-1] in all_count_words:\n",
    "        feat[all_count_index[r_ls[-1]]]+=1\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2=[]\n",
    "for r in review:\n",
    "    feat=feature2(r)\n",
    "    features2.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(features2, labels)\n",
    "prediction2=clf.predict(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9683580241694552"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE2=mean_squared_error(labels, prediction2)\n",
    "MSE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the MSE is 0.968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse document frequency of the words ‘stories’, ‘magician’, ‘psychic’, ‘writing’, and ‘won- der’\n",
    "#What are their tf-idf scores in the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "idf_dic=defaultdict(float)\n",
    "for r in review:\n",
    "    r_ls=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    list_words=['stories','magician', 'psychic', 'writing', 'wonder']\n",
    "    for w in r_ls:\n",
    "        if w in list_words:\n",
    "            idf_dic[w]+=1\n",
    "            list_words.remove(w)  \n",
    "for w in list_words:\n",
    "    idf_dic[w]=np.log10(len(review)/idf_dic[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idf score for ['stories','magician', 'psychic', 'writing', 'wonder'] in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=review[0]\n",
    "list_words=['stories','magician', 'psychic', 'writing', 'wonder']\n",
    "ls=''.join([i.lower() for i in list(example['review_text']) if i not in punctuation]).split()\n",
    "tf=[]\n",
    "for i in list_words:\n",
    "    ct=ls.count(i)\n",
    "    tf.append(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.11747546, 2.65757732, 5.20411998, 0.99783394, 1.76700389])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ls=np.array(tf)*np.array(list(idf_dic.values()))\n",
    "tfidf_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf score for the first review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(73431, 'the'), (44301, 'and'), (39577, 'a')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf scores for the 1000 most common unigrams as features\n",
    "#MSE\n",
    "uni_counts=[(unigramDic[x],x) for x in unigramDic]\n",
    "uni_counts.sort(reverse=True)\n",
    "count=uni_counts[0:1000]\n",
    "count[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordID=dict(zip([x[1] for x in count], range(len(count))))\n",
    "word=[x[1] for x in count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf for word\n",
    "idf_dic=defaultdict(float)\n",
    "for r in review:\n",
    "    r_ls=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    words=[x[1] for x in count]\n",
    "    for w in r_ls:\n",
    "        if w in words:\n",
    "            idf_dic[w]+=1\n",
    "            words.remove(w)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in word:\n",
    "    \n",
    "    idf_dic[w]=np.log10(len(review)/(idf_dic[w]+1))\n",
    "idf={k: idf_dic[k] for k in word}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat3(r):\n",
    "    feat=[0]*1000\n",
    "    tf_dic=defaultdict(int)\n",
    "    r_ls=''.join([i.lower() for i in list(r['review_text']) if i not in punctuation]).split()\n",
    "    for i in r_ls:\n",
    "        if i in word:\n",
    "            tf_dic[i]+=1\n",
    "    tf={k: tf_dic[k] for k in word}\n",
    "    for i in range(1000):\n",
    "        feat[i]=tf[word[i]]*idf[word[i]]\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature3=[]\n",
    "for r in review:\n",
    "    feat=feat3(r)\n",
    "    feature3.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(feature3, labels)\n",
    "pred3=clf.predict(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660142273530128"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE3=mean_squared_error(labels, pred3)\n",
    "MSE3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mse is 0.966\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which other review has the highest cosine similarity compared to the first review, in terms of their \n",
    "#tf-idf representations (considering unigrams only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=np.array(feature3[0])\n",
    "r=np.array(feature3[1:])\n",
    "def cs(r1,r2):\n",
    "    numer=np.dot(r1, r2)\n",
    "    denum=(np.dot(r1,r1)*np.dot(r2,r2))**(1/2)\n",
    "    return numer/denum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b9835c2a2a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcs_ls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcs_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcs_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "cs_ls=[]\n",
    "for i in range(len(r)):\n",
    "    cos=cs(r1,r[i])\n",
    "    cs_ls.append(cos)\n",
    "index=cs_ls.index(max(cs_ls))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r81495268\n",
      "This review is going to be very personal. I will probably not talk about the writing, the plot or the characters the way that I normally would. Dear Martin is about 17-year-old Justyce McAllistera a highly successful black student who attends a primarily all-white private school. One day, he is helping a drunk friend and arrested. In response, he starts writing letters to Dr, Martin Luther King Jr. as a way to cope with his newly sharpened awareness of the world around him \n",
      " The world needed this book right now. This is a book that every school needs. I am going to be buying copies for friends and family. I am a teacher. I can see how this book would be an excellent way to help students start having these difficult conversations. Because until we teach young people how to recognize and talk about racism, sexism, homophobia, and xenophobia and whatever else needs to get talked about nothing will change. They will unconsciously internalize the system's biases. They won't see the things that need to change. And that the things that need to change start with themselves. \n",
      " I enjoyed the writing. There are quite a few conversations that are almost written like a script. Unusual but not distracting and I think that the convention highlighted the importance of what people were saying. I was fully investing the characters. So invested that if I don't get regular updates about Justyce and assurances that he is thriving and well on his way to making the impact on the world that he deserves I am going to be so mad. Aside from Justyce, the standouts for me were Manny, SJ, Jared, and Doc. \n",
      " One thing that seems to keep coming up is this sense of estrangement from self. Everyone in this novel (and most likely life) has this idea of who they are that doesn't quite fit with the reality of who they are. There are these ideas and visions that other people have about them that they don't know how to reconcile themselves with. Justyce struggles to reconcile the two sides of his life, Jared doesn't recognize his internalized racism, and Manny overlooks \n",
      " Justyce starts to talk about \"the Black Man's Curse\" which I hadn't heard about before. As I understand it, the BMC is the phenomenon that however well a black man does, however successful he may be they are never able to get to a point where they don't have to worry about racism. Or about their race affecting their life. That someone is always waiting to slap you down, tread on your work, and assume that you don't deserve your success because you are a black man. Just the short term second-hand frustration of reading about it was overwhelming. I cannot imagine what it is like to have to live with and deal with it for your entire life. How strong do you have to be to push through all that is holding you back? And what does it say about our society that we expect that? \n",
      " So read this book. It is well written, it is important, and it is going to stay with you a long time after you finish the final page.\n"
     ]
    }
   ],
   "source": [
    "print(review[index]['review_id'])\n",
    "print(review[index]['review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffle the data\n",
    "index=random.sample(range(len(reviews)),30000)\n",
    "train_index=index[:10000]\n",
    "valid_index=index[10000:20000]\n",
    "test_index=index[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train, validation and test data\n",
    "train=[]\n",
    "for i in train_index:\n",
    "    train.append(reviews[i])\n",
    "valid=[]\n",
    "for i in valid_index:\n",
    "    valid.append(reviews[i])\n",
    "test=[]\n",
    "for i in test_index:\n",
    "    test.append(reviews[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[train, valid, test]\n",
    "y=[]\n",
    "y_v=[]\n",
    "y_t=[]\n",
    "Y=[y, y_v, y_t]\n",
    "train_text=[]\n",
    "valid_text=[]\n",
    "test_text=[]\n",
    "raw_text=[train_text, valid_text, test_text]\n",
    "for index in range(len(data)):\n",
    "    for r in data[index]:\n",
    "        rate=r['rating']\n",
    "        Y[index].append(rate)\n",
    "        r=list(''.join([i.lower() for i in list(r['review_text'])]))\n",
    "        for n in range(len(r)):\n",
    "            if r[n] in punctuation:\n",
    "                r[n]=str(' '+r[n]+' ')\n",
    "        r=''.join(r).split()\n",
    "        raw_text[index].append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni(train_text, valid_text, test_text, ifCount, ifRemove):\n",
    "    train_features=[]\n",
    "    val_features=[]\n",
    "    test_features=[]\n",
    "    features_ls=[train_features, val_features, test_features]\n",
    "    text_ls=[train_text, valid_text, test_text]\n",
    "    if ifRemove:\n",
    "        for text in text_ls:\n",
    "            for n in range(len(text)):\n",
    "                text[n]=[i for i in text[n] if i not in punctuation]\n",
    "                \n",
    "     \n",
    "    #get the topwords of training data for unigram \n",
    "    train_dic=defaultdict(int)\n",
    "    for r in train_text:\n",
    "        for word in r:\n",
    "            train_dic[word]+=1\n",
    "    wordCount=[(train_dic[x],x) for x in train_dic]\n",
    "    wordCount.sort(reverse=True)\n",
    "    topwords=[x[1] for x in wordCount[:1000]]\n",
    "    topwordsID=dict(zip(topwords, range(1000)))\n",
    "       \n",
    "    \n",
    "    for i in range(3):\n",
    "        for r in text_ls[i]:\n",
    "            feat=[0]*1000\n",
    "            for word in r:\n",
    "                if word in topwords:\n",
    "                    feat[topwordsID[word]]+=1\n",
    "            features_ls[i].append(feat)\n",
    "\n",
    "\n",
    "         \n",
    "    if ifCount==False: #tdidf=tf*idf\n",
    "        #get the idf for topwords of training data for unigram\n",
    "        idf_uni=[0]*1000\n",
    "        for r in train_text:\n",
    "            words=topwords\n",
    "            for w in list(set([i for i in r if i in words])):\n",
    "                idf_uni[topwordsID[w]]+=1\n",
    "        idf_uni=np.log10(len(train_text)/np.array(idf_uni))\n",
    "         \n",
    "        for i in range(3):\n",
    "            feat_ls=features_ls[i]\n",
    "            for n in range(len(feat_ls)):\n",
    "                feat_ls[n]=feat_ls[n]*idf_uni\n",
    "    return features_ls\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi(train_text, valid_text, test_text, ifCount, ifRemove):\n",
    "    train_features=[]\n",
    "    val_features=[]\n",
    "    test_features=[]\n",
    "    features_ls=[train_features, val_features, test_features]\n",
    "    text_ls=[train_text, valid_text, test_text]\n",
    "    \n",
    "    if ifRemove:\n",
    "        for text in text_ls:\n",
    "            for n in range(len(text)):\n",
    "                text[n]=[i for i in text[n] if i not in punctuation]\n",
    "    \n",
    "    #convert train, valid, test data to bigram\n",
    "    for i in range(3):\n",
    "        for r in text_ls[i]:\n",
    "            r_ls=[]\n",
    "            index=text_ls[i].index(r)\n",
    "            for n in range(len(r)-1):\n",
    "                string=str(r[n]+' '+r[n+1])\n",
    "                r_ls.append(string)\n",
    "            text_ls[i][index]=r_ls\n",
    "    \n",
    "    \n",
    "     \n",
    "    #get the topwords of training data for unigram \n",
    "    train_dic=defaultdict(int)\n",
    "    for r in train_text:\n",
    "        for string in r:\n",
    "            train_dic[string]+=1\n",
    "    wordCount=[(train_dic[x],x) for x in train_dic]\n",
    "    wordCount.sort(reverse=True)\n",
    "    topwords=[x[1] for x in wordCount[:1000]]\n",
    "    topwordsID=dict(zip(topwords, range(1000))) \n",
    "\n",
    "        \n",
    "    #count\n",
    "    for i in range(3):\n",
    "        for r in text_ls[i]:\n",
    "            feat=[0]*1000\n",
    "            for word in r:\n",
    "                if word in topwords:\n",
    "                    feat[topwordsID[word]]+=1\n",
    "            features_ls[i].append(feat)\n",
    "\n",
    "\n",
    "         \n",
    "    if ifCount==False: #tdidf=tf*idf\n",
    "        #get the idf for topwords of training data for unigram\n",
    "        idf_bi=[0]*1000\n",
    "        for r in train_text:\n",
    "            words=topwords\n",
    "            for w in list(set([i for i in r if i in words])):\n",
    "                idf_bi[topwordsID[w]]+=1\n",
    "        idf_bi=np.log10(len(train_text)/np.array(idf_bi))\n",
    "         \n",
    "        for i in range(3):\n",
    "            feat_ls=features_ls[i]\n",
    "            for n in range(len(feat_ls)):\n",
    "                feat_ls[n]=feat_ls[n]*idf_bi\n",
    "    return features_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr=[]\n",
    "X_v=[]\n",
    "X_t=[]\n",
    "####uni\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#uni remove count\n",
    "ls=uni(tr, v, t, True, True)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#uni remove tfidf\n",
    "ls=uni(tr, v, t, False, True)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#uni preserve count\n",
    "ls=uni(tr, v, t, True, False)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#uni preserve tfidf\n",
    "ls=uni(tr, v, t, False, False)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "####bi\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#bi remove count\n",
    "ls=bi(tr, v, t, True, True)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#bi remove tfidf\n",
    "ls=bi(tr, v, t, False, True)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#bi preserve count\n",
    "ls=bi(tr, v, t, True, False)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "tr,v,t=train_text, valid_text, test_text\n",
    "#bi preserve tfidf\n",
    "ls=bi(tr, v, t, False, False)\n",
    "X_tr.append(ls[0])\n",
    "X_v.append(ls[1])\n",
    "X_t.append(ls[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in range(8):\n",
    "    MSE=[]\n",
    "    parameters=[0.01, 0.1, 1, 10, 100]\n",
    "    for c in parameters:\n",
    "        model=linear_model.Ridge(c, fit_intercept=False)\n",
    "        model.fit(X_tr[i], y)\n",
    "        prediction=model.predict(X_v[i])\n",
    "        mse_v=mean_squared_error(y_v, prediction)\n",
    "        MSE.append(mse_v)\n",
    "    bestC=parameters[MSE.index(max(MSE))]\n",
    "    clf=linear_model.Ridge(bestC, fit_intercept=False)\n",
    "    clf.fit(X_tr[i],y)\n",
    "    pred=clf.predict(X_t[i])\n",
    "    mse=mean_squared_error(y_t, pred)\n",
    "    result.append(mse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.503796908591908,\n",
       " 7.5038681552506645,\n",
       " 7.503796908591908,\n",
       " 7.5038681552506645,\n",
       " 8.104043962118938,\n",
       " 8.913924376101305,\n",
       " 10.207440563059665,\n",
       " 11.81079622343473]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uni remove count': 7.503796908591908,\n",
       " 'uni remove tfidf': 7.5038681552506645,\n",
       " 'uni preserve count': 7.503796908591908,\n",
       " 'uni preserve tfidf': 7.5038681552506645,\n",
       " 'bi remove count': 8.104043962118938,\n",
       " 'bi remove tfidf': 8.913924376101305,\n",
       " 'bi preserve count': 10.207440563059665,\n",
       " 'bi preserve tfidf': 11.81079622343473}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram=[\"uni\", \"bi\"]\n",
    "remove=[\"remove\", \"preserve\"]\n",
    "count=[\"count\",\"tfidf\"]\n",
    "strings=[]\n",
    "for g in gram:\n",
    "    for r in remove:\n",
    "        for c in count:\n",
    "            string=str(g+' '+r+' '+c)\n",
    "            strings.append(string)\n",
    "\n",
    "            \n",
    "dict(zip(strings, result))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uni remove count</th>\n",
       "      <th>uni remove tfidf</th>\n",
       "      <th>uni preserve count</th>\n",
       "      <th>uni preserve tfidf</th>\n",
       "      <th>bi remove count</th>\n",
       "      <th>bi remove tfidf</th>\n",
       "      <th>bi preserve count</th>\n",
       "      <th>bi preserve tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.503797</td>\n",
       "      <td>7.503868</td>\n",
       "      <td>7.503797</td>\n",
       "      <td>7.503868</td>\n",
       "      <td>8.104044</td>\n",
       "      <td>8.913924</td>\n",
       "      <td>10.207441</td>\n",
       "      <td>11.810796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uni remove count  uni remove tfidf  uni preserve count  uni preserve tfidf  \\\n",
       "0          7.503797          7.503868            7.503797            7.503868   \n",
       "\n",
       "   bi remove count  bi remove tfidf  bi preserve count  bi preserve tfidf  \n",
       "0         8.104044         8.913924          10.207441          11.810796  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(dict(zip(strings, result)), index=range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
